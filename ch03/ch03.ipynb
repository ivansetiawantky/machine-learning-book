{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning with PyTorch and Scikit-Learn  \n",
    "# -- Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package version checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add folder to path in order to load from the check_packages.py script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check recommended package versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_environment_check import check_packages\n",
    "\n",
    "\n",
    "d = {\"numpy\": \"1.21.2\", \"matplotlib\": \"3.4.3\", \"sklearn\": \"1.0\", \"pandas\": \"1.3.2\"}\n",
    "check_packages(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - A Tour of Machine Learning Classifiers Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Choosing a classification algorithm](#Choosing-a-classification-algorithm)\n",
    "- [First steps with scikit-learn](#First-steps-with-scikit-learn)\n",
    "    - [Training a perceptron via scikit-learn](#Training-a-perceptron-via-scikit-learn)\n",
    "- [Modeling class probabilities via logistic regression](#Modeling-class-probabilities-via-logistic-regression)\n",
    "    - [Logistic regression intuition and conditional probabilities](#Logistic-regression-intuition-and-conditional-probabilities)\n",
    "    - [Learning the weights of the logistic loss function](#Learning-the-weights-of-the-logistic-loss-function)\n",
    "    - [Training a logistic regression model with scikit-learn](#Training-a-logistic-regression-model-with-scikit-learn)\n",
    "    - [Tackling overfitting via regularization](#Tackling-overfitting-via-regularization)\n",
    "- [Maximum margin classification with support vector machines](#Maximum-margin-classification-with-support-vector-machines)\n",
    "    - [Maximum margin intuition](#Maximum-margin-intuition)\n",
    "    - [Dealing with the nonlinearly separable case using slack variables](#Dealing-with-the-nonlinearly-separable-case-using-slack-variables)\n",
    "    - [Alternative implementations in scikit-learn](#Alternative-implementations-in-scikit-learn)\n",
    "- [Solving nonlinear problems using a kernel SVM](#Solving-nonlinear-problems-using-a-kernel-SVM)\n",
    "    - [Using the kernel trick to find separating hyperplanes in higher dimensional space](#Using-the-kernel-trick-to-find-separating-hyperplanes-in-higher-dimensional-space)\n",
    "- [Decision tree learning](#Decision-tree-learning)\n",
    "    - [Maximizing information gain – getting the most bang for the buck](#Maximizing-information-gain-–-getting-the-most-bang-for-the-buck)\n",
    "    - [Building a decision tree](#Building-a-decision-tree)\n",
    "    - [Combining weak to strong learners via random forests](#Combining-weak-to-strong-learners-via-random-forests)\n",
    "- [K-nearest neighbors – a lazy learning algorithm](#K-nearest-neighbors-–-a-lazy-learning-algorithm)\n",
    "- [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a classification algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Iris dataset from scikit-learn. Here, the third column represents the petal length, and the fourth column the petal width of the flower examples. The classes are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.data.shape)\n",
    "X = iris.data[:, [2, 3]]\n",
    "print(X.shape)\n",
    "y = iris.target\n",
    "\n",
    "print(\"Class labels:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into 70% training and 30% test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels counts in y:\", np.bincount(y))\n",
    "print(\"Labels counts in y_train:\", np.bincount(y_train))\n",
    "print(\"Labels counts in y_test:\", np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a perceptron via scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=1)\n",
    "ppn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ppn.predict(X_test_std)\n",
    "print(\"Misclassified examples: %d\" % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy: %.3f\" % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %.3f\" % ppn.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To check recent matplotlib compatibility\n",
    "import matplotlib\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = (\"o\", \"s\", \"^\", \"v\", \"<\")\n",
    "    colors = (\"red\", \"blue\", \"lightgreen\", \"gray\", \"cyan\")\n",
    "    cmap = ListedColormap(colors[: len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)\n",
    "    )\n",
    "    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    lab = lab.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class examples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(\n",
    "            x=X[y == cl, 0],\n",
    "            y=X[y == cl, 1],\n",
    "            alpha=0.8,\n",
    "            c=colors[idx],\n",
    "            marker=markers[idx],\n",
    "            label=f\"Class {cl}\",\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "\n",
    "    # highlight test examples\n",
    "    if test_idx:\n",
    "        # plot all examples\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c=\"none\",\n",
    "            edgecolor=\"black\",\n",
    "            alpha=1.0,\n",
    "            linewidth=1,\n",
    "            marker=\"o\",\n",
    "            s=100,\n",
    "            label=\"Test set\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a perceptron model using the standardized training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "\n",
    "plot_decision_regions(\n",
    "    X=X_combined_std, y=y_combined, classifier=ppn, test_idx=range(105, 150)\n",
    ")\n",
    "plt.xlabel(\"Petal length [standardized]\")\n",
    "plt.ylabel(\"Petal width [standardized]\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_01.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling class probabilities via logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression intuition and conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "z = np.arange(-7, 7, 0.1)\n",
    "sigma_z = sigmoid(z)\n",
    "\n",
    "plt.plot(z, sigma_z)\n",
    "plt.axvline(0.0, color=\"k\")\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"$\\\\sigma (z)$\")\n",
    "\n",
    "# y axis ticks and gridline\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "ax = plt.gca()\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_02.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figures/03_03.png\", width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figures/03_25.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the weights of the logistic loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_1(z):\n",
    "    return -np.log(sigmoid(z))\n",
    "\n",
    "\n",
    "def loss_0(z):\n",
    "    return -np.log(1 - sigmoid(z))\n",
    "\n",
    "\n",
    "z = np.arange(-10, 10, 0.1)\n",
    "sigma_z = sigmoid(z)\n",
    "\n",
    "c1 = [loss_1(x) for x in z]\n",
    "plt.plot(sigma_z, c1, label=\"L(w, b) if y=1\")\n",
    "\n",
    "c0 = [loss_0(x) for x in z]\n",
    "plt.plot(sigma_z, c0, linestyle=\"--\", label=\"L(w, b) if y=0\")\n",
    "\n",
    "plt.ylim(0.0, 5.1)\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel(\"$\\sigma(z)$\")\n",
    "plt.ylabel(\"L(w, b)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_04.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD:\n",
    "    \"\"\"Gradient descent-based logistic regression classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after training.\n",
    "    b_ : Scalar\n",
    "      Bias unit after fitting.\n",
    "    losses_ : list\n",
    "       Log loss function values in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : Instance of LogisticRegressionGD\n",
    "\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = np.float64(0.0)\n",
    "        self.losses_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = y - output\n",
    "            self.w_ += self.eta * X.T.dot(errors) / X.shape[0]\n",
    "            self.b_ += self.eta * errors.mean()\n",
    "            loss = (-y.dot(np.log(output)) - (1 - y).dot(np.log(1 - output))) / X.shape[\n",
    "                0\n",
    "            ]\n",
    "            self.losses_.append(loss)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "\n",
    "    def activation(self, z):\n",
    "        \"\"\"Compute logistic sigmoid activation\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(z, -250, 250)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_01_subset = X_train_std[(y_train == 0) | (y_train == 1)]\n",
    "y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]\n",
    "\n",
    "lrgd = LogisticRegressionGD(eta=0.3, n_iter=1000, random_state=1)\n",
    "lrgd.fit(X_train_01_subset, y_train_01_subset)\n",
    "\n",
    "plot_decision_regions(X=X_train_01_subset, y=y_train_01_subset, classifier=lrgd)\n",
    "\n",
    "plt.xlabel(\"Petal length [standardized]\")\n",
    "plt.ylabel(\"Petal width [standardized]\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_05.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a logistic regression model with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# lr = LogisticRegression(C=100.0, solver=\"lbfgs\", multi_class=\"ovr\")\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "lr = OneVsRestClassifier(LogisticRegression(C=100.0, solver=\"lbfgs\"))\n",
    "# lr = OneVsRestClassifier(LogisticRegression(C=100.0, solver=\"newton-cg\"))\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# # multinomial\n",
    "# lr = LogisticRegression(C=100.0, solver=\"lbfgs\")\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "plot_decision_regions(\n",
    "    X_combined_std, y_combined, classifier=lr, test_idx=range(105, 150)\n",
    ")\n",
    "plt.xlabel(\"Petal length [standardized]\")\n",
    "plt.ylabel(\"Petal width [standardized]\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_06.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# import sklearn\n",
    "# print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict_proba(X_test_std[:4, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict_proba(X_test_std[:4, :]).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict_proba(X_test_std[:4, :]).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X_test_std[:4, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_std[0, :].shape)\n",
    "print(X_test_std[0, :].reshape(1, -1).shape)\n",
    "print(X_test_std[0:1, :].shape)\n",
    "print(X_test_std[0:5, :].shape)\n",
    "\n",
    "lr.predict(X_test_std[0, :].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tackling overfitting via regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figures/03_07.png\", width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "weights, params = [], []\n",
    "for c in np.arange(-5, 5):\n",
    "    lr = LogisticRegression(C=10.0**c, multi_class=\"ovr\")\n",
    "    # lr = OneVsRestClassifier(LogisticRegression(C=10.0**c, solver=\"lbfgs\"))\n",
    "    lr.fit(X_train_std, y_train)\n",
    "    weights.append(lr.coef_[1])\n",
    "    params.append(10.0**c)\n",
    "\n",
    "weights = np.array(weights)\n",
    "plt.plot(params, weights[:, 0], label=\"Petal length\")\n",
    "plt.plot(params, weights[:, 1], linestyle=\"--\", label=\"Petal width\")\n",
    "plt.ylabel(\"Weight coefficient\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xscale(\"log\")\n",
    "# plt.savefig('figures/03_08.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum margin classification with support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figures/03_09.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum margin intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with the nonlinearly separable case using slack variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figures/03_10.png\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel=\"linear\", C=1.0, random_state=1)\n",
    "svm.fit(X_train_std, y_train)\n",
    "\n",
    "plot_decision_regions(\n",
    "    X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150)\n",
    ")\n",
    "plt.xlabel(\"Petal length [standardized]\")\n",
    "plt.ylabel(\"Petal width [standardized]\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_11.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative implementations in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "ppn = SGDClassifier(loss=\"perceptron\")\n",
    "lr = SGDClassifier(loss=\"log\")\n",
    "svm = SGDClassifier(loss=\"hinge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving non-linear problems using a kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "X_xor = np.random.randn(200, 2)\n",
    "y_xor = np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0)\n",
    "y_xor = np.where(y_xor, 1, 0)\n",
    "\n",
    "plt.scatter(\n",
    "    X_xor[y_xor == 1, 0],\n",
    "    X_xor[y_xor == 1, 1],\n",
    "    c=\"royalblue\",\n",
    "    marker=\"s\",\n",
    "    label=\"Class 1\",\n",
    ")\n",
    "plt.scatter(\n",
    "    X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], c=\"tomato\", marker=\"o\", label=\"Class 0\"\n",
    ")\n",
    "\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_12.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figures/03_13.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the kernel trick to find separating hyperplanes in higher dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"rbf\", random_state=1, gamma=0.10, C=10.0)\n",
    "svm.fit(X_xor, y_xor)\n",
    "plot_decision_regions(X_xor, y_xor, classifier=svm)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_14.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel=\"rbf\", random_state=1, gamma=0.2, C=1.0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "\n",
    "plot_decision_regions(\n",
    "    X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150)\n",
    ")\n",
    "plt.xlabel(\"Petal length [standardized]\")\n",
    "plt.ylabel(\"Petal width [standardized]\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_15.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"rbf\", random_state=1, gamma=100.0, C=1.0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "\n",
    "plot_decision_regions(\n",
    "    X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150)\n",
    ")\n",
    "plt.xlabel(\"Petal length [standardized]\")\n",
    "plt.ylabel(\"Petal width [standardized]\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_16.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figures/03_17.png\", width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return -p * np.log2(p) - (1 - p) * np.log2((1 - p))\n",
    "\n",
    "\n",
    "x = np.arange(0.0, 1.0, 0.01)\n",
    "ent = [entropy(p) if p != 0 else None for p in x]\n",
    "\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.xlabel(\"Class-membership probability p(i=1)\")\n",
    "plt.plot(x, ent)\n",
    "# plt.savefig('figures/03_26.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figures/03_18.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing information gain - getting the most bang for the buck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gini(p):\n",
    "    return p * (1 - p) + (1 - p) * (1 - (1 - p))\n",
    "\n",
    "\n",
    "def entropy(p):\n",
    "    return -p * np.log2(p) - (1 - p) * np.log2((1 - p))\n",
    "\n",
    "\n",
    "def error(p):\n",
    "    return 1 - np.max([p, 1 - p])\n",
    "\n",
    "\n",
    "x = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "ent = [entropy(p) if p != 0 else None for p in x]\n",
    "sc_ent = [e * 0.5 if e else None for e in ent]\n",
    "err = [error(i) for i in x]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "for (\n",
    "    i,\n",
    "    lab,\n",
    "    ls,\n",
    "    c,\n",
    ") in zip(\n",
    "    [ent, sc_ent, gini(x), err],\n",
    "    [\"Entropy\", \"Entropy (scaled)\", \"Gini impurity\", \"Misclassification error\"],\n",
    "    [\"-\", \"-\", \"--\", \"-.\"],\n",
    "    [\"black\", \"lightgray\", \"red\", \"green\", \"cyan\"],\n",
    "):\n",
    "    line = ax.plot(x, i, label=lab, linestyle=ls, lw=2, color=c)\n",
    "\n",
    "ax.legend(\n",
    "    loc=\"upper center\", bbox_to_anchor=(0.5, 1.15), ncol=5, fancybox=True, shadow=False\n",
    ")\n",
    "\n",
    "ax.axhline(y=0.5, linewidth=1, color=\"k\", linestyle=\"--\")\n",
    "ax.axhline(y=1.0, linewidth=1, color=\"k\", linestyle=\"--\")\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xlabel(\"p(i=1)\")\n",
    "plt.ylabel(\"Impurity index\")\n",
    "# plt.savefig('figures/03_19.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=4, random_state=1)\n",
    "tree_model.fit(X_train, y_train)\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision_regions(\n",
    "    X_combined, y_combined, classifier=tree_model, test_idx=range(105, 150)\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Petal length [cm]\")\n",
    "plt.ylabel(\"Petal width [cm]\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_20.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# feature_names = [\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\"]\n",
    "feature_names = [\"Petal length\", \"Petal width\"]\n",
    "tree.plot_tree(tree_model, feature_names=feature_names, filled=True)\n",
    "\n",
    "# plt.savefig('figures/03_21_1.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining weak to strong learners via random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=25, random_state=1, n_jobs=2)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "plot_decision_regions(\n",
    "    X_combined, y_combined, classifier=forest, test_idx=range(105, 150)\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Petal length [cm]\")\n",
    "plt.ylabel(\"Petal width [cm]\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbors - a lazy learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figures/03_23.png\", width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric=\"minkowski\")\n",
    "knn.fit(X_train_std, y_train)\n",
    "\n",
    "plot_decision_regions(\n",
    "    X_combined_std, y_combined, classifier=knn, test_idx=range(105, 150)\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Petal length [standardized]\")\n",
    "plt.ylabel(\"Petal width [standardized]\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/03_24_figures.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Readers may ignore the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python ../.convert_notebook_to_script.py --input ch03.ipynb --output ch03.py\n",
    "#\n",
    "# Use jupytext to convert ipynb to py, as below. OR just use vscode Export\n",
    "! jupytext --to py:percent ch03.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pymlbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
